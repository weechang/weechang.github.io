---
title: 论如何优雅地使用爬虫摸鱼
date: 2018-09-21 16:05:11
categories: 日常记录
tags: [爬虫, Jsoup]
---

>最近手上项目空了下来，没什么事做。博客博客不想写，文章文章不想看。于是干脆看点小说吧，但是上班时间，大家都在认认真真敲代码，自己拿出手机看小说又不是很好（其实主要是数据线坏了，在公司没发充电），电脑上浏览器看，更是不行。于是想了想，干脆就自己爬着看吧，把内容打印在IDE的控制台，想一想这波操作就很骚，于是说动就动。

<!-- more -->

# 爬虫选择

由于本人是一枚正经的Javaer，所以爬虫当然也要用Java咯。Java下也有几款比较好的爬虫软件，如nutch、crawler4j 等。但是我只是爬个网页，看个小说而已啊。于是就选了个Jsoup，直接解析Html信息，从中提取小说内容。

其实选择Jsoup还有个原因就是我好歹写过一阵子jQuery，对jQuery语法比较熟悉。因为Jsoup语法与jQuery语法非常一致。

# 开始动工

添加maven依赖
<pre>

&lt;dependency>
    &lt;groupId> org.jsoup &lt;/groupId>
    &lt;artifactId> jsoup &lt;/artifactId>
    &lt;version> 1.9.2 &lt;/version>
&lt;/dependency>

</pre>
    
# 爬取页面信息

Jsoup的爬取方式十分简单，是通过获取html文档到本地，然后再用jQuery的解析方式做的DOM解析。

<pre>
public class BiQuGeCrawler extends AbstractCrawler {

    @Override
    public String getPage(String url) {
        try {
            page = Jsoup.connect(url).get();

            this.getNext();
            this.getLast();
        } catch (IOException e) {
            e.printStackTrace();
        }
        return this.getContent();
    }

    @Override
    protected String getContent(){
        Element cntEl = page.getElementById("content");
        // 八个空格，（制表符号）
        return cntEl.text().replaceAll("        ", "\n");
    }

    protected void getNext() {
        Element ul = page.getElementsByClass("page_chapter").get(0).child(0);
        Element nextHref = ul.child(2).child(0);
        nextUrl = nextHref.attr("abs:href");
    }

    protected void getLast() {
        Element ul = page.getElementsByClass("page_chapter").get(0).child(0);
        Element lastHref = ul.child(0).child(0);
        lastUrl = lastHref.attr("abs:href");
    }
}
</pre>

获取小说正文内容及前一页、后一页链接等关键信息。

# 设置翻页及退出

每次抓取完页面后，监听控制台输入值，进行翻页、退出操作。

<pre>
public class Function {

    // app配置
    private AppConfig config;
    // 爬虫类
    private AbstractCrawler crawler;

    public Function(String firstUrl){
        config = new AppConfig();
        crawler =  CrawlerFactory.build(config.sourceType);
        startView(firstUrl);
    }

    // 页面浏览
    private void startView(String pageUrl){
        String content = crawler.getPage(pageUrl);
        System.out.println(content);
        this.inputListener();
    }

    // 开始浏览
    private void inputListener(){
        System.out.println("*************");
        System.out.println("* L 上一页   *");
        System.out.println("* Q 退出     *");
        System.out.println("* 其他 下一页 *");
        System.out.println("*************");
        Scanner sc = new Scanner(System.in);
        String input = sc.nextLine();
        if ("l".equalsIgnoreCase(input)){
            // 上一页
            startView(crawler.lastUrl);
        } else if ("q".equalsIgnoreCase(input)){
            // 退出
        } else {
            // 下一页
            startView(crawler.nextUrl);
        }
    }
}

</pre>

如上，整个摸鱼神器的关键代码就已经完成了，具体的完整代码，可以查看[我的github项目](https://github.com/weechang/ReadingCrawler)

# Run一下

首先配置需要看的小说网页信息及个人操作习惯设置。然后通过运行main方法。即可运行。

{% asset_img Run.png 效果预览 %}

# 后续支持

OK，到此爬虫已经能够正常爬取小说内容了。并且已经实现翻页、退出等基本功能。后续将支持更多小说来源如 **纵横综合网** 等网站。以及更多的功能如 **日志混淆**、 **日志格式化** 、**断点续看** 等功能。

**本文章涉及的代码已托管到github，欢迎各位客官使用[https://github.com/weechang/ReadingCrawler](https://github.com/weechang/ReadingCrawler)**